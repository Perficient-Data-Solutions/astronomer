
# TCP input
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

# HTTP input
<source>
  @type http
  port 8888
</source>

# modified grab from hosts
<source>
  @type tail
  @id fluentd-containers.log
  path /var/lib/docker/containers/*/*.log
  pos file /var/log/fluentd-containers/*/*.log
  tag raw.kubernetes.*
  read_from_head true
  <parse>
    @type json
    json_parser json
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</source>

# match fluent logs
<match fluent.**>
  @type null
</match>

<match kubernetes.var.log.containers.**fluentd**.log>
  @type null
</match>

<match kubernetes.var.log.containers.**kube-system**.log>
  @type null
</match>

# Detect exceptions in the log output and forward them as one log entry
# Remove tag_prefix raw
<match raw.kubernetes.**>
  @id raw.kubernetes
  @type detect_exceptions
  remove_tag_prefix raw
  message ''
  stream stream
  multiline_flush_interval 5
  max_bytes 500000
  max_lines 1000
</match>

# Enrich records with Kubernetes metadata
# <filter kubernetes.**>
#   @type kubernetes_metadata
# </filter>

# Filter for workers, ignore all else, this will continue downstream
# <match kubernetes.**>
#   @type copy
#   <store>
#     @type forward
#   </store>

# <label @WORKER>
#   <filter kubernetes.**>
#     @type grep
#     <regexp>
#       key $.kubernetes.namespace_name
#       pattern "^#{ENV['NAMESPACE']}-.*$"
#     </regexp>
#     <regexp>
#       key $.kubernetes.labels.component
#       pattern ^(worker)$
#     </regexp>
#   </filter>
# </label>
#
# <filter kubernetes.**>
#   @type grep
#   <regexp>
#     key
#   <regexp>
#     key $.kubernetes.labels.component
#     pattern ^(worker)$
#   </regexp>
# </filter>


<filter kubernetes.**>
  @type parser
  <parse>
    @type json
    json_parser json
  </parse>
  replace_invalid_sequence true
  emit_invalid_record_to_error false
  key_name log
  reserve_data true
</filter>


<filter kubernetes.**>
  @type record_transformer
  enable_ruby
  # renew_record
  # keep_keys message dag_id task_id execution_date try_number log_id offset
  <record>
    message ${record["message"]}
    # log_id ${record["dag_id"]}-${record["task_id"]}-${record["execution_date"]&.gsub("_plus_", "+")&.gsub("_", ":")&.gsub(" ", "T")}-${record["try_number"]}
    log_id ${record["dag_id"]}_${record["task_id"]}_${record["execution_date"]&.gsub("_plus_", "+")&.gsub(" ", "T")&.gsub("-", "_")&.gsub(":", "_")}_${record["try_number"]}
    offset ${time = Time.now; time.to_i * (10 ** 9) + time.nsec}
  </record>
</filter>

# Send off to elasticsearch

<match kubernetes.**>
  @type elasticsearch
  @log_level info
  include_timestamp true
  host elasticsearch
  port 9200
  reconnect_on_error true
  index_name fluentd
  <buffer>
    @type file
    path "/var/log/fluentd-buffers/kubernetes.system.buffer"
    flush_mode interval
    retry_type exponential_backoff
    flush_thread_count 2
    flush_interval 5s
    retry_forever
    retry_max_interval 30
    overflow_action block
  </buffer>
</match>
