# Core
AIRFLOW__CORE__PARALLELISM=5
AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
AIRFLOW__CORE__EXECUTOR=CeleryExecutor

# Specify config class path
AIRFLOW__CORE__LOGGING_CONFIG_CLASS=airflow.config.log_config.LOGGING_CONFIG

# New in 1.10
# AIRFLOW__CORE__FAB_LOGGING_LEVEL = WARN
# AIRFLOW__CORE__LOG_FILENAME_TEMPLATE = {{{{ ti.dag_id }}}}/{{{{ ti.task_id }}}}/{{{{ ts }}}}/{{{{ try_number }}}}.log
# AIRFLOW__CORE__LOG_PROCESSOR_FILENAME_TEMPLATE = {{{{ filename }}}}.log

# Only need to change this to use elasticsearch or file task handler
AIRFLOW__CORE__TASK_LOG_READER=elasticsearch


# Specify remote logging
AIRFLOW__CORE__REMOTE_LOGGING=True

# Celery settings
AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres:5432/airflow

# Scheduler settings
AIRFLOW__SCHEDULER__STATSD_ON=True
AIRFLOW__SCHEDULER__STATSD_HOST=statsd-exporter
AIRFLOW__SCHEDULER__STATSD_PORT=9125
AIRFLOW__SCHEDULER__STATSD_PREFIX=airflow

# Elasticsearch
AIRFLOW__ELASTICSEARCH__ELASTICSEARCH_HOST=elasticsearch:9200
# If we set this to false, we can use the file task handler instead?
AIRFLOW__ELASTICSEARCH__ELASTICSEARCH_WRITE_STDOUT=True

AIRFLOW__ELASTICSEARCH__END_OF_LOG_MARK= end_of_log
AIRFLOW__ELASTICSEARCH__JSON_FORMAT=True
